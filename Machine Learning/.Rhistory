#Works on tested operating system (Windows 7). Please change values if needed.
}
#Load libraries
library(dplyr)
library(ggplot2)
library(lattice)
library(caret)
library(e1071)
library(randomForest)
library(rpart)
library(rpart.plot)
#Load the training and testing data
trainingdata <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testingdata <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#Divide the training set in a training and testing set
inTrain <- createDataPartition(y=trainingdata$classe,
p=0.7, list=FALSE)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
#Cleaning and transformations
#Removing the x variable (# observation)
training <- training[c(-1)]
testing <- testing[c(-1)]
trainingdata <- trainingdata[c(-1)]
#Removing variables with many NA's (treshold: 70% NA)
training2 <- data.frame(matrix(nrow=nrow(training)))
trainingnames <- character()
a <- numeric()
#Training set
for(i in 1:length(training)){
if(sum(is.na(training[1:nrow(training), i]))/nrow(training) <= 0.3){
training2 <- cbind(training2, training[,i])
trainingnames <- append(trainingnames, names(training[i]))
a <- append(a,i)
}
}
training2 <- training2[,-1]
names(training2) <- trainingnames
training <- training2
#Apply the same transformation to the test set
test2 <- data.frame(matrix(nrow=nrow(testing)))
for(j in 1:length(a)){
test2 <- cbind(test2, testing[,a[j]])
}
test2 <- test2[,-1]
names(test2) <- trainingnames
testing <- test2
#Apply the same transformation to the final test set
test3 <- data.frame(matrix(nrow=nrow(testingdata)))
for(k in 1:length(a)){
test3 <- cbind(test3, testingdata[,a[k]])
}
test3 <- test3[,-1]
names(test3) <- trainingnames
testingdata <- test3
#Develop the model
#Run the random forest algoritm on the training set
model <- randomForest(classe ~. , data=(training))
#Test the model
#Run the model on the testing set (derived from the training data)
predictions <- predict(model, testing, type = "class")
#Check the accuracy of the model
confusionMatrix(predictions, testing$classe)
#Predict the test data
predictions2 <- predict(model, testingdata, type = "class")
pml_write_files = function(x){
n = length(x)
for(k in 1:n){
filename = paste0("problem_id_",k,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictions2)
test3 <- data.frame(matrix(nrow=nrow(testingdata)))
nrow(testingdata)
View(testingdata)
#Download the files (if not already done)
if (!file.exists("pml-training.csv")) {
url  = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
dest = "pml-training.csv"
meth = "internal"
quit = TRUE
mode = "wb"
download.file(url, dest, meth, quit, mode)
#Works on tested operating system (Windows 7). Please change values if needed.
}
if (!file.exists("pml-testing.csv")) {
url  = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
dest = "pml-testing.csv"
meth = "internal"
quit = TRUE
mode = "wb"
download.file(url, dest, meth, quit, mode)
#Works on tested operating system (Windows 7). Please change values if needed.
}
#Load libraries
library(dplyr)
library(ggplot2)
library(lattice)
library(caret)
library(e1071)
library(randomForest)
library(rpart)
library(rpart.plot)
#Load the training and testing data
trainingdata <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testingdata <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#Divide the training set in a training and testing set
inTrain <- createDataPartition(y=trainingdata$classe,
p=0.7, list=FALSE)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
#Cleaning and transformations
#Removing the x variable (# observation)
training <- training[c(-1)]
testing <- testing[c(-1)]
testingdata <- testingdata[c(-1)]
#Removing variables with many NA's (treshold: 70% NA)
training2 <- data.frame(matrix(nrow=nrow(training)))
trainingnames <- character()
a <- numeric()
#Training set
for(i in 1:length(training)){
if(sum(is.na(training[1:nrow(training), i]))/nrow(training) <= 0.3){
training2 <- cbind(training2, training[,i])
trainingnames <- append(trainingnames, names(training[i]))
a <- append(a,i)
}
}
training2 <- training2[,-1]
names(training2) <- trainingnames
training <- training2
#Apply the same transformation to the test set
test2 <- data.frame(matrix(nrow=nrow(testing)))
for(j in 1:length(a)){
test2 <- cbind(test2, testing[,a[j]])
}
test2 <- test2[,-1]
names(test2) <- trainingnames
testing <- test2
#Apply the same transformation to the final test set
test3 <- data.frame(matrix(nrow=nrow(testingdata)))
for(k in 1:length(a)){
test3 <- cbind(test3, testingdata[,a[k]])
}
test3 <- test3[,-1]
names(test3) <- trainingnames
testingdata <- test3
#Develop the model
#Run the random forest algoritm on the training set
model <- randomForest(classe ~. , data=(training))
#Test the model
#Run the model on the testing set (derived from the training data)
predictions <- predict(model, testing, type = "class")
#Check the accuracy of the model
confusionMatrix(predictions, testing$classe)
#Predict the test data
predictions2 <- predict(model, testingdata, type = "class")
pml_write_files = function(x){
n = length(x)
for(k in 1:n){
filename = paste0("problem_id_",k,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictions2)
View(testingdata)
View(trainingdata)
View(training)
#Load the training and testing data
trainingdata <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testingdata <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#Divide the training set in a training and testing set
inTrain <- createDataPartition(y=trainingdata$classe,
p=0.7, list=FALSE)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
#Cleaning and transformations
#Removing the x variable (# observation)
training <- training[c(-1)]
testing <- testing[c(-1)]
testingdata <- testingdata[c(-1)]
#Removing variables with many NA's (treshold: 70% NA)
training2 <- data.frame(matrix(nrow=nrow(training)))
trainingnames <- character()
a <- numeric()
#Training set
for(i in 1:length(training)){
if(sum(is.na(training[1:nrow(training), i]))/nrow(training) <= 0.3){
training2 <- cbind(training2, training[,i])
trainingnames <- append(trainingnames, names(training[i]))
a <- append(a,i)
}
}
training2 <- training2[,-1]
names(training2) <- trainingnames
training <- training2
#Apply the same transformation to the test set
test2 <- data.frame(matrix(nrow=nrow(testing)))
for(j in 1:length(a)){
test2 <- cbind(test2, testing[,a[j]])
}
test2 <- test2[,-1]
names(test2) <- trainingnames
testing <- test2
#Apply the same transformation to the final test set
test3 <- data.frame(matrix(nrow=nrow(testingdata)))
for(k in 1:length(a)){
test3 <- cbind(test3, testingdata[,a[k]])
}
View(test3)
test3 <- test3[,-1]
View(test2)
View(test3)
names(trainingdata)==names(testingdata)
View(testingdata)
View(trainingdata)
#Load the training and testing data
trainingdata <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testingdata <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#Divide the training set in a training and testing set
inTrain <- createDataPartition(y=trainingdata$classe,
p=0.7, list=FALSE)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
#Cleaning and transformations
#Removing the x variable (# observation)
training <- training[c(-1)]
testing <- testing[c(-1)]
testingdata <- testingdata[c(-1)]
names(training)==names(testingdata)
View(testingdata)
#Load the training and testing data
trainingdata <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testingdata <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#Divide the training set in a training and testing set
inTrain <- createDataPartition(y=trainingdata$classe,
p=0.7, list=FALSE)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
View(testingdata)
names(training)==names(testingdata)
View(trainingdata)
View(training)
names(training)
names(testingdata)
View(test3)
test3 <- test3[,-1]
names(test3) <- trainingnames
View(test3)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
#Cleaning and transformations
#Removing the x variable (# observation)
training <- training[c(-1)]
testing <- testing[c(-1)]
testingdata <- testingdata[c(-1)]
#Removing variables with many NA's (treshold: 70% NA)
training2 <- data.frame(matrix(nrow=nrow(training)))
trainingnames <- character()
a <- numeric()
#Training set
for(i in 1:length(training)){
if(sum(is.na(training[1:nrow(training), i]))/nrow(training) <= 0.3){
training2 <- cbind(training2, training[,i])
trainingnames <- append(trainingnames, names(training[i]))
a <- append(a,i)
}
}
training2 <- training2[,-1]
names(training2) <- trainingnames
training <- training2
#Apply the same transformation to the test set
test2 <- data.frame(matrix(nrow=nrow(testing)))
for(j in 1:length(a)){
test2 <- cbind(test2, testing[,a[j]])
}
test2 <- test2[,-1]
names(test2) <- trainingnames
testing <- test2
#Apply the same transformation to the final test set
test3 <- data.frame(matrix(nrow=nrow(testingdata)))
for(k in 1:length(a)){
test3 <- cbind(test3, testingdata[,a[k]])
}
test3 <- test3[,-1]
names(test3) <- trainingnames
View(test3)
View(trainingdata)
names(test3[59]) <- names(testingdata[160])
names(test3[59]) <- names(testingdata[159])
View(test3)
names(testingdata[159])
names(test3[59])
names(test3[59]) <- "problem_id"
View(test3)
names(test3[59])
names(test3[59]) <- c("problem_id")
names(test3[59])
colnames(test3[59]) <- names(testingdata[159])
names(test3[59])
colnames(test3)[59] <- names(testingdata[159])
names(test3[59])
names(training)==names(testingdata)
names(training)==names(test3)
#Download the files (if not already done)
if (!file.exists("pml-training.csv")) {
url  = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
dest = "pml-training.csv"
meth = "internal"
quit = TRUE
mode = "wb"
download.file(url, dest, meth, quit, mode)
#Works on tested operating system (Windows 7). Please change values if needed.
}
if (!file.exists("pml-testing.csv")) {
url  = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
dest = "pml-testing.csv"
meth = "internal"
quit = TRUE
mode = "wb"
download.file(url, dest, meth, quit, mode)
#Works on tested operating system (Windows 7). Please change values if needed.
}
#Load libraries
library(dplyr)
library(caret)
library(randomForest)
#Load the training and testing data
trainingdata <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testingdata <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#Divide the training set in a training and testing set
inTrain <- createDataPartition(y=trainingdata$classe,
p=0.7, list=FALSE)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
#Cleaning and transformations
#Removing the x variable (# observation)
training <- training[c(-1)]
testing <- testing[c(-1)]
testingdata <- testingdata[c(-1)]
#Removing variables with many NA's (treshold: 70% NA)
training2 <- data.frame(matrix(nrow=nrow(training)))
trainingnames <- character()
a <- numeric()
#Training set
for(i in 1:length(training)){
if(sum(is.na(training[1:nrow(training), i]))/nrow(training) <= 0.3){
training2 <- cbind(training2, training[,i])
trainingnames <- append(trainingnames, names(training[i]))
a <- append(a,i)
}
}
training2 <- training2[,-1]
names(training2) <- trainingnames
training <- training2
#Apply the same transformation to the test set
test2 <- data.frame(matrix(nrow=nrow(testing)))
for(j in 1:length(a)){
test2 <- cbind(test2, testing[,a[j]])
}
test2 <- test2[,-1]
names(test2) <- trainingnames
testing <- test2
#Apply the same transformation to the final test set
test3 <- data.frame(matrix(nrow=nrow(testingdata)))
for(k in 1:length(a)){
test3 <- cbind(test3, testingdata[,a[k]])
}
test3 <- test3[,-1]
names(test3) <- trainingnames
colnames(test3)[59] <- names(testingdata[159])
testingdata <- test3
#Develop the model
#Run the random forest algoritm on the training set
model <- randomForest(classe ~. , data=(training))
#Test the model
#Run the model on the testing set (derived from the training data)
predictions <- predict(model, testing, type = "class")
#Check the accuracy of the model
confusionMatrix(predictions, testing$classe)
#Predict the test data
predictions2 <- predict(model, testingdata, type = "class")
pml_write_files = function(x){
n = length(x)
for(k in 1:n){
filename = paste0("problem_id_",k,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictions2)
source('~/.active-rstudio-document')
install.packages("rattle")
install.packages("rattle")
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
set.seed(12345)
#Load the training and testing data
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
getwd()
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[c(-1)]
trainingV3 <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
for(j in 1:length(trainingV3)) {
if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
trainingV3 <- trainingV3[ , -j] #Remove that column
}
}
}
}
#To check the new N?? of observations
dim(trainingV3)
myTraining <- trainingV3
rm(trainingV3)
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[clean1]
testing <- testing[clean2]
#To check the new N?? of observations
dim(myTesting)
dim(testing)
for (i in 1:length(testing) ) {
for(j in 1:length(myTraining)) {
if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
class(testing[j]) <- class(myTraining[i])
}
}
}
#And to make sure Coertion really worked, simple smart ass technique:
testing <- rbind(myTraining[2, -58] , testing) #note row 2 does not mean anything, this will be removed right.. now:
testing <- testing[-1,]
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
confusionMatrix(predictionsA1, myTesting$classe)
modFitB1 <- randomForest(classe ~. , data=myTraining)
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
confusionMatrix(predictionsB1, myTesting$classe)
predictionsB2 <- predict(modFitB1, testing, type = "class")
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictionsB2)
library(data.table)
library(caret)
library(ggplot2)
library(doMC)
library(knitr)
library(xtable)
library(randomForest)
registerDoMC(cores = 7)
## You should create a report describing how you built your model, how you used cross validation,
## what you think the expected out of sample error is, and why you made the choices you did.
## You will also use your prediction model to predict 20 different test cases.
## 1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML
## file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number
## of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages
## branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).
## 2. You should also apply your machine learning algorithm to the 20 test cases available in the test data
## above. Please submit your predictions in appropriate format to the programming assignment for automated grading.
## See the programming assignment for additional details.
download.pml <- function() {
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv")
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv")
}
read.pml <- function(file) {
fread(file, na.strings=c("#DIV/0!", ""))
}
build.report <- function() {
knit2html("project.Rmd", "index.html")
}
raw.train <- read.pml("pml-training.csv")
raw.validation <- read.pml("pml-testing.csv")
set.seed(13)
## contains some NA values
na.cols <- raw.train[,sapply(.SD, function(x) any(is.na(x)))]
drop.columns <- function(x) {
x[,!c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window"),with=F]
}
transform.features <- function(x) {
x[,classe:=factor(classe)]
}
## try only columns that have values
training.features <- drop.columns(raw.train[,eval(names(which(na.cols == F))),with=F])
write.pml.predictions <- function(x) {
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
submit.prediction <- function(x, validation) {
in.train <- createDataPartition(x$classe, p=.60, list=FALSE)
train <- x[in.train[,1]]
model.rf <- train(y=as.factor(train$classe), x=train[,!"classe",with=F], tuneGrid=data.frame(mtry=3), trControl=trainControl(method="none"), method="parRF")
write.pml.predictions(predict(model.rf, newdata=drop.columns(validation[,eval(names(which(na.cols == F))[-60]),with=F])))
}
